from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request
from fastapi.responses import Response
from fastapi.middleware.cors import CORSMiddleware
import json
import base64
import uvicorn
from datetime import datetime, timezone
import os
import re
import audioop
import wave
import webrtcvad
import openai
from dotenv import load_dotenv
from gcal import get_current_event, book_next_available

# Import database functions
from database.db_actions import init_db, add_row, Voicemail

load_dotenv()

# Initialize database
SQLITE_URL = os.getenv("SQLITECLOUD_URL")
if SQLITE_URL:
    try:
        init_db(SQLITE_URL)
        print("‚úÖ Database initialized successfully")
    except Exception as e:
        print(f"‚ö†Ô∏è Database initialization failed: {e}")
        SQLITE_URL = None
else:
    print("‚ö†Ô∏è SQLITECLOUD_URL not set - database features disabled")

HTTP_SERVER_PORT = 8080
RECORDINGS_DIR = "recordings"
TRANSCRIPTS_DIR = "transcripts"
KEVIN_PHONE_NUMBER = os.getenv("PERSONAL_PHONE")  # Set in .env file

# VAD configuration
VAD_MODE = 2           # 0-3, 3=most aggressive
FRAME_MS = 20          # must be 10/20/30 ms
END_SIL_MS = 1000      # silence threshold to end utterance (1.5 seconds - wait for caller to finish)
MAX_UTT_MS = 20000     # max utterance length
MIN_SPEECH_MS = 500    # minimum speech duration to count as valid utterance (ignore breath/noise)
MIN_EXCHANGES_BEFORE_ACTION = 0
VOICE = "belinda"

# Echo/Delay configuration (tune these to adjust timing)
POST_AUDIO_DELAY_SECONDS = 0.5   # Fixed delay after bot audio finishes playing before accepting user input

# Create recordings directory if it doesn't exist
os.makedirs(RECORDINGS_DIR, exist_ok=True)
os.makedirs(TRANSCRIPTS_DIR, exist_ok=True)

app = FastAPI()

def log(msg, *args):
    print(f"Media WS: {msg}", *args)


# Initialize BosonAI clients with multiple API keys for cycling
boson_clients = []
boson_api_keys = []

# Load all available API keys (BOSONAI_API_KEY1, BOSONAI_API_KEY2, BOSONAI_API_KEY3)
for i in range(1, 6):
    api_key = os.getenv(f"BOSONAI_API_KEY{i}")
    if api_key:
        boson_api_keys.append(api_key)
        boson_clients.append(openai.Client(
            api_key=api_key,
            base_url="https://hackathon.boson.ai/v1"
        ))
        log(f"Loaded BosonAI API key #{i}")

# Fallback to single BOSONAI_API_KEY if numbered keys not found
if not boson_clients and os.getenv("BOSONAI_API_KEY"):
    boson_api_keys.append(os.getenv("BOSONAI_API_KEY"))
    boson_clients.append(openai.Client(
        api_key=os.getenv("BOSONAI_API_KEY"),
        base_url="https://hackathon.boson.ai/v1"
    ))
    log("Loaded BosonAI API key (legacy)")
# Add CORS middleware to allow frontend to access the API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Keep single client reference for backward compatibility
boson_client = boson_clients[0] if boson_clients else None

# API key cycling state
current_key_index = 0
API_REQUEST_TIMEOUT = 10.0  # seconds - timeout for BosonAI requests

# Lock to ensure sequential API calls within same conversation turn
import asyncio
api_call_lock = asyncio.Lock()


async def call_bosonai_with_retry(func_name: str, use_lock: bool = True, **kwargs):
    """
    Call a BosonAI function with automatic key cycling on timeout.
    Tries each available API key in sequence until one succeeds or all fail.
    
    Args:
        func_name: Name of the function to call (e.g., 'chat.completions.create', 'audio.speech.create')
        use_lock: Whether to use the lock to ensure sequential calls (default: True)
        **kwargs: Arguments to pass to the BosonAI function
    
    Returns:
        The response from BosonAI, or None if all keys fail
    """
    global current_key_index
    
    if not boson_clients:
        log("[BosonAI not configured - set BOSONAI_API_KEY1/2/3 env vars]")
        return None
    
    import asyncio
    
    # Use lock to ensure sequential calls within same conversation turn use same API key
    async def _make_call():
        global current_key_index
        
        # Try each API key in sequence
        num_keys = len(boson_clients)
        for attempt in range(num_keys):
            key_idx = (current_key_index + attempt) % num_keys
            client = boson_clients[key_idx]
            
            try:
                log(f"Attempting BosonAI call with API key #{key_idx + 1} (timeout: {API_REQUEST_TIMEOUT}s)...")
                
                # Parse the function path (e.g., "chat.completions.create" -> client.chat.completions.create)
                func = client
                for part in func_name.split('.'):
                    func = getattr(func, part)
                
                # Call the function with timeout
                response = await asyncio.wait_for(
                    asyncio.to_thread(func, **kwargs),
                    timeout=API_REQUEST_TIMEOUT
                )
                
                # Success! Update current key index for next call
                if key_idx != current_key_index:
                    log(f"‚úÖ Switched to API key #{key_idx + 1} successfully")
                    current_key_index = key_idx
                
                return response
                
            except asyncio.TimeoutError:
                log(f"‚è±Ô∏è API key #{key_idx + 1} timed out after {API_REQUEST_TIMEOUT}s")
                if attempt < num_keys - 1:
                    log(f"üîÑ Trying next API key...")
                continue
                
            except Exception as e:
                log(f"‚ùå API key #{key_idx + 1} failed: {e}")
                if attempt < num_keys - 1:
                    log(f"üîÑ Trying next API key...")
                continue
        
        # All keys failed
        log(f"‚ùå All {num_keys} API keys failed")
        return None
    
    # Execute with or without lock
    if use_lock:
        async with api_call_lock:
            return await _make_call()
    else:
        return await _make_call()


def extract_caller_name(conversation_history: list, latest_transcription: str) -> str:
    """
    Extract caller's name from conversation history.
    Looks for patterns like "This is [name]", "My name is [name]", "[name] calling", etc.
    """
    # Combine all caller messages
    all_caller_text = ""
    for msg in conversation_history:
        if msg.get("role") == "user":
            all_caller_text += msg.get("content", "") + " "
    
    all_caller_text += latest_transcription
    
    # Simple name extraction patterns
    patterns = [
        r"(?:this is|it's|i'm|my name is)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)",
        r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s+(?:calling|here)",
        r"(?:^|\s)([A-Z][a-z]+\s+[A-Z][a-z]+)(?:\s|$)",  # Two capitalized words
    ]
    
    for pattern in patterns:
        match = re.search(pattern, all_caller_text, re.IGNORECASE)
        if match:
            name = match.group(1).strip()
            # Filter out common false positives
            false_positives = ["Kevin", "Good Morning", "Good Afternoon", "Thank You"]
            if name not in false_positives and len(name) > 2:
                return name
    
    # Default if no name found
    return "Unknown Caller"


def clean_text_for_transcript(text: str) -> str:
    """
    Clean text for transcript logging by removing:
    - <think>...</think> tags (AI reasoning)
    - Content in brackets: [text], (text), {text}
    """
    import re
    # Remove <think> tags and their content
    cleaned = re.sub(r'<think>.*?</think>\s*', '', text, flags=re.DOTALL)
    # Remove bracketed content
    cleaned = re.sub(r'\[.*?\]|\(.*?\)|\{.*?\}', '', cleaned)
    # Clean up extra whitespace
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


def save_transcript(call_sid: str, from_number: str, conversation_log: list, call_start_time: datetime, call_end_time: datetime, final_action: str = None, call_in_progress: bool = False):
    """Save call transcript to JSON file. Can be called multiple times to update the same file."""
    try:
        timestamp = call_start_time.strftime("%Y%m%d_%H%M%S")
        filename = f"{TRANSCRIPTS_DIR}/transcript_{call_sid}_{timestamp}.json"
        
        # Calculate call duration
        duration_seconds = (call_end_time - call_start_time).total_seconds()
        
        # Build transcript data
        transcript_data = {
            "call_sid": call_sid,
            "from_number": from_number,
            "start_time": call_start_time.isoformat(),
            "end_time": call_end_time.isoformat(),
            "duration_seconds": round(duration_seconds, 2),
            "call_in_progress": call_in_progress,  # True if call is still active
            "final_action": final_action,  # "FORWARD", "END", or None
            "conversation": []
        }
        
        # Process conversation log
        for entry in conversation_log:
            if entry['speaker'] == 'Caller':
                caller_text = entry.get('text', None)
                transcript_data['conversation'].append({
                    "speaker": "Caller",
                    "duration_ms": entry.get('duration_ms', 0),
                    "audio_size_bytes": entry.get('audio_size', 0),
                    "text": caller_text if caller_text else "[Transcription not available]"
                })
            else:  # Bot
                transcript_data['conversation'].append({
                    "speaker": "AI Receptionist",
                    "text": entry.get('text', ''),
                    "timestamp": entry.get('timestamp', '')
                })
        
        # Save to JSON file
        import json as json_module
        with open(filename, 'w', encoding='utf-8') as f:
            json_module.dump(transcript_data, f, indent=2, ensure_ascii=False)
        
        log(f"üíæ Transcript saved: {filename}")
        
        # Also save a human-readable text version
        txt_filename = f"{TRANSCRIPTS_DIR}/transcript_{call_sid}_{timestamp}.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"CALL TRANSCRIPT\n")
            f.write(f"{'='*60}\n")
            f.write(f"Call ID: {call_sid}\n")
            f.write(f"From: {from_number}\n")
            f.write(f"Start: {call_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"End: {call_end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Duration: {duration_seconds:.1f} seconds\n")
            if call_in_progress:
                f.write(f"Status: CALL IN PROGRESS (transcript updating in real-time)\n")
            if final_action:
                action_text = "Forwarded to Kevin" if final_action == "FORWARD" else "Ended (Spam Detected)"
                f.write(f"Result: {action_text}\n")
            f.write(f"{'='*60}\n\n")
            
            for i, entry in enumerate(conversation_log, 1):
                if entry['speaker'] == 'Caller':
                    caller_text = entry.get('text', None)
                    duration = entry.get('duration_ms', 0)
                    if caller_text:
                        f.write(f"[{i}] CALLER: {caller_text}\n\n")
                    else:
                        f.write(f"[{i}] CALLER: [Spoke for {duration}ms - transcription unavailable]\n\n")
                else:
                    text = entry.get('text', '')
                    f.write(f"[{i}] AI RECEPTIONIST: {text}\n\n")
        
        log(f"üìÑ Human-readable transcript saved: {txt_filename}")
        
    except Exception as e:
        log(f"Error saving transcript: {e}")
        import traceback
        traceback.print_exc()

# Initialize BosonAI clients with multiple API keys for cycling
boson_clients = []
boson_api_keys = []

# Load all available API keys (BOSONAI_API_KEY1, BOSONAI_API_KEY2, BOSONAI_API_KEY3)
for i in range(1, 6):
    api_key = os.getenv(f"BOSONAI_API_KEY{i}")
    if api_key:
        boson_api_keys.append(api_key)
        boson_clients.append(openai.Client(
            api_key=api_key,
            base_url="https://hackathon.boson.ai/v1"
        ))
        log(f"Loaded BosonAI API key #{i}")

# Fallback to single BOSONAI_API_KEY if numbered keys not found
if not boson_clients and os.getenv("BOSONAI_API_KEY"):
    boson_api_keys.append(os.getenv("BOSONAI_API_KEY"))
    boson_clients.append(openai.Client(
        api_key=os.getenv("BOSONAI_API_KEY"),
        base_url="https://hackathon.boson.ai/v1"
    ))
    log("Loaded BosonAI API key (legacy)")

# Keep single client reference for backward compatibility
boson_client = boson_clients[0] if boson_clients else None

# API key cycling state
current_key_index = 0
API_REQUEST_TIMEOUT = 10.0  # seconds - timeout for BosonAI requests

# Lock to ensure sequential API calls within same conversation turn
import asyncio
api_call_lock = asyncio.Lock()


async def call_bosonai_with_retry(func_name: str, use_lock: bool = True, **kwargs):
    """
    Call a BosonAI function with automatic key cycling on timeout.
    Tries each available API key in sequence until one succeeds or all fail.
    
    Args:
        func_name: Name of the function to call (e.g., 'chat.completions.create', 'audio.speech.create')
        use_lock: Whether to use the lock to ensure sequential calls (default: True)
        **kwargs: Arguments to pass to the BosonAI function
    
    Returns:
        The response from BosonAI, or None if all keys fail
    """
    global current_key_index
    
    if not boson_clients:
        log("[BosonAI not configured - set BOSONAI_API_KEY1/2/3 env vars]")
        return None
    
    import asyncio
    
    # Use lock to ensure sequential calls within same conversation turn use same API key
    async def _make_call():
        global current_key_index
        
        # Try each API key in sequence
        num_keys = len(boson_clients)
        for attempt in range(num_keys):
            key_idx = (current_key_index + attempt) % num_keys
            client = boson_clients[key_idx]
            
            try:
                log(f"Attempting BosonAI call with API key #{key_idx + 1} (timeout: {API_REQUEST_TIMEOUT}s)...")
                
                # Parse the function path (e.g., "chat.completions.create" -> client.chat.completions.create)
                func = client
                for part in func_name.split('.'):
                    func = getattr(func, part)
                
                # Call the function with timeout
                response = await asyncio.wait_for(
                    asyncio.to_thread(func, **kwargs),
                    timeout=API_REQUEST_TIMEOUT
                )
                
                # Success! Update current key index for next call
                if key_idx != current_key_index:
                    log(f"‚úÖ Switched to API key #{key_idx + 1} successfully")
                    current_key_index = key_idx
                
                return response
                
            except asyncio.TimeoutError:
                log(f"‚è±Ô∏è API key #{key_idx + 1} timed out after {API_REQUEST_TIMEOUT}s")
                if attempt < num_keys - 1:
                    log(f"üîÑ Trying next API key...")
                continue
                
            except Exception as e:
                log(f"‚ùå API key #{key_idx + 1} failed: {e}")
                if attempt < num_keys - 1:
                    log(f"üîÑ Trying next API key...")
                continue
        
        # All keys failed
        log(f"‚ùå All {num_keys} API keys failed")
        return None
    
    # Execute with or without lock
    if use_lock:
        async with api_call_lock:
            return await _make_call()
    else:
        return await _make_call()


def mulaw8k_to_pcm16_16k(mulaw_bytes: bytes) -> bytes:
    """Decode Œº-law 8 kHz ‚Üí PCM16 8 kHz, then upsample ‚Üí PCM16 16 kHz."""
    pcm16_8k = audioop.ulaw2lin(mulaw_bytes, 2)                      # 2 bytes/sample
    pcm16_16k, _ = audioop.ratecv(pcm16_8k, 2, 1, 8000, 16000, None) # inwidth=2, nchannels=1
    return pcm16_16k


def pcm16_16k_to_mulaw8k(pcm16_16k: bytes) -> bytes:
    """Downsample PCM16 16 kHz ‚Üí 8 kHz, then encode to Œº-law for Twilio."""
    pcm16_8k, _ = audioop.ratecv(pcm16_16k, 2, 1, 16000, 8000, None)
    return audioop.lin2ulaw(pcm16_8k, 2)


async def get_call_info(call_sid: str) -> dict:
    """
    Retrieve detailed information about a call including AI-generated summary.
    
    Returns a dictionary with:
        id: str - Call SID
        number: str - Caller phone number
        name: str - Extracted caller name
        description: str - AI-generated summary of the call
        spam: bool - Whether call was marked as spam
        date: datetime - Call start time
        unread: bool - Whether call has been reviewed (always False for now)
        recording: str - Path to WAV recording file
        transcript: str - Path to transcript JSON file
    """
    import glob
    import json as json_module
    from pathlib import Path
    
    try:
        # Find transcript file for this call_sid
        transcript_pattern = f"{TRANSCRIPTS_DIR}/transcript_{call_sid}_*.json"
        transcript_files = glob.glob(transcript_pattern)
        
        if not transcript_files:
            log(f"No transcript found for call {call_sid}")
            return None
        
        # Use the most recent transcript if multiple exist
        transcript_path = sorted(transcript_files)[-1]
        
        # Load transcript data
        with open(transcript_path, 'r', encoding='utf-8') as f:
            transcript_data = json_module.load(f)
        
        # Extract basic info
        call_id = transcript_data.get('call_sid', call_sid)
        from_number = transcript_data.get('from_number', 'Unknown')
        start_time_str = transcript_data.get('start_time')
        start_time = datetime.fromisoformat(start_time_str) if start_time_str else datetime.now()
        final_action = transcript_data.get('final_action')
        is_spam = final_action == "END"
        
        # Extract caller name from conversation
        caller_name = "Unknown Caller"
        conversation = transcript_data.get('conversation', [])
        for entry in conversation:
            if entry.get('speaker') == 'Caller' and entry.get('text'):
                # Try to extract name from first caller message
                import re
                text = entry.get('text', '')
                patterns = [
                    r"(?:this is|it's|i'm|my name is)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)",
                    r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s+(?:calling|here)",
                ]
                for pattern in patterns:
                    match = re.search(pattern, text, re.IGNORECASE)
                    if match:
                        name = match.group(1).strip()
                        if len(name) > 2:
                            caller_name = name
                            break
                if caller_name != "Unknown Caller":
                    break
        
        # Find corresponding recording file
        recording_pattern = f"{RECORDINGS_DIR}/call_{call_sid}_*.wav"
        recording_files = glob.glob(recording_pattern)
        recording_path = sorted(recording_files)[-1] if recording_files else None
        
        # Generate AI summary of the conversation
        description = await generate_call_summary(conversation)
        
        # Build result dictionary
        result = {
            "id": call_id,
            "number": from_number,
            "name": caller_name,
            "description": description,
            "spam": is_spam,
            "date": start_time,
            "unread": False,  # Could be implemented with a separate tracking system
            "recording": recording_path,
            "transcript": transcript_path
        }
        
        return result
        
    except Exception as e:
        log(f"Error getting call info: {e}")
        import traceback
        traceback.print_exc()
        return None


async def generate_call_summary(conversation: list) -> str:
    """
    Generate a concise summary of the call conversation using AI.
    
    Args:
        conversation: List of conversation entries from transcript
        
    Returns:
        A brief summary string describing the call
    """
    if not boson_client or not conversation:
        return "No conversation data available"
    
    try:
        # Build a readable conversation text
        conversation_text = ""
        for entry in conversation:
            speaker = entry.get('speaker', 'Unknown')
            text = entry.get('text', '[No text]')
            conversation_text += f"{speaker}: {text}\n"
        
        # Prompt the AI to summarize
        summary_prompt = f"""Summarize the following phone call conversation in 1-2 sentences. Focus on:
- Who called and why
- What action was taken (forwarded, booked meeting, or ended as spam)

Conversation:
{conversation_text}

Provide only the summary, nothing else."""
        
        response = await call_bosonai_with_retry(
            "chat.completions.create",
            model="Qwen3-14B-Hackathon",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that creates concise call summaries."
                },
                {
                    "role": "user",
                    "content": summary_prompt
                }
            ],
            temperature=0.3,  # Lower temperature for consistent summaries
        )
        
        if response and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            # Remove any thinking tags
            import re
            summary = re.sub(r'<think>.*?</think>\s*', '', summary, flags=re.DOTALL).strip()
            return summary
        else:
            return "Unable to generate summary"
            
    except Exception as e:
        log(f"Error generating summary: {e}")
        return "Error generating summary"


async def process_utterance_and_respond(pcm16_16k: bytes, websocket: WebSocket, stream_sid: str, conversation_history: list, call_sid: str, wav_file=None, exchange_count: int = 0):
    """Send PCM16 16kHz audio to BosonAI and stream response back to Twilio."""
    if not boson_client:
        log("[BosonAI not configured - set BOSONAI_API_KEY env var]")
        return None, None, None, None
    
    try:
        # Save audio chunk to temporary WAV file (following example1.py pattern)
        import io
        
        # Create WAV file in memory
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wav:
            wav.setnchannels(1)
            wav.setsampwidth(2)
            wav.setframerate(16000)
            wav.writeframes(pcm16_16k)
        
        wav_data = wav_buffer.getvalue()
        audio_base64 = base64.b64encode(wav_data).decode("utf-8")
        
        log("Step 1: Transcribing caller audio...")
        
        # STEP 1: Transcribe the audio first
        transcription_messages = [
            {
                "role": "system",
                "content": "You are a transcription assistant. Transcribe the audio exactly as spoken. Only output the transcription text, nothing else."
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": audio_base64,
                            "format": "wav",
                        },
                    },
                ],
            }
        ]
        
        transcription_response = await call_bosonai_with_retry(
            "chat.completions.create",
            model="higgs-audio-understanding-Hackathon",
            messages=transcription_messages,
            temperature=0.3,  # Lower temperature for accurate transcription
        )
        
        if not transcription_response:
            log("Failed to get transcription from BosonAI (all API keys failed or timed out)")
            return None, None, None, None
        
        caller_transcription = transcription_response.choices[0].message.content
        
        if not caller_transcription:
            log("No transcription received")
            return None, None, None, None
        
        log(f"üìù Caller said: \"{caller_transcription}\"")
        
        # STEP 2: Use transcription to generate response
        log("Step 2: Generating AI response from transcription...")
        
        # Get current calendar event status
        current_event_info = get_current_event()
        log(f"üìÖ Calendar status: {current_event_info}")
        
        # Build messages with conversation history
        calendar_context = ""
        if current_event_info and current_event_info != "No current events":
            calendar_context = f"\n\nCALENDAR STATUS:\nKevin is currently in: {current_event_info}\n\nFor legitimate callers, if Kevin is busy, offer to book them at the next available time instead of forwarding. Use the command BOOK_MEETING on a new line after your response."
        else:
            calendar_context = f"\n\nCALENDAR STATUS:\nKevin is available right now (no current meetings).\n\nFor legitimate callers, you can forward them directly."
        
        messages = [
            {
                "role": "system",
                # ...existing code...
                "content": f"""You are Kevin's professional AI receptionist. Your core mission: protect Kevin's valuable time while ensuring legitimate callers receive excellent service.

PRIMARY RESPONSIBILITIES:
1. Greet callers warmly but professionally - be helpful, not desperate
2. Quickly identify caller's name and purpose through strategic questioning
3. Make high-confidence decisions: legitimate business vs. spam/scam
4. Route appropriately: forward, book, gather more info, or politely dismiss

SPAM DETECTION - End call immediately if caller:
‚Ä¢ Mentions classic scams: car warranty, IRS threats, tech support, debt relief, student loans, free vacations, "final notice" warnings, extended warranties
‚Ä¢ Uses aggressive, threatening, or high-pressure language
‚Ä¢ Refuses to provide name, company, or clear purpose after 2 requests
‚Ä¢ Has obviously robotic voice, pre-recorded messages, or scripted sales pitches
‚Ä¢ Claims to represent government agencies demanding immediate payment
‚Ä¢ Offers unrequested services (solar, insurance, home security, etc.)
‚Ä¢ Makes vague threats or creates false urgency ("act now or lose...")
‚Ä¢ Has background noise typical of call centers (many voices, beeping)

LEGITIMATE CALL INDICATORS:
‚Ä¢ Knows Kevin personally or professionally (mentions past interactions)
‚Ä¢ References specific projects, deliverables, mutual contacts, or previous conversations
‚Ä¢ Has well-articulated business inquiry with concrete details
‚Ä¢ Professional, respectful, patient communication style
‚Ä¢ Provides clear company/organization affiliation
‚Ä¢ Purpose aligns with Kevin's known work or interests
‚Ä¢ Willing to provide contact information for callback

NEED MORE INFORMATION - Gather details when caller:
‚Ä¢ Mentions potentially valuable opportunity but lacks specifics
‚Ä¢ References a project/topic Kevin might be interested in but seems unclear
‚Ä¢ Claims to have been referred by someone but you need to verify legitimacy
‚Ä¢ Has a reasonable purpose but hasn't provided their name yet
‚Ä¢ Situation is ambiguous - could be legitimate business or sophisticated sales
‚Ä¢ You're 60-80% confident they're legitimate but need one more data point

When gathering more info, ask targeted questions:
- "Can you tell me which project this is regarding?"
- "Who referred you to Kevin?"
- "What company are you with?"
- "Can you be more specific about what you'd like to discuss?"

CONVERSATION STYLE:
‚Ä¢ Warm but efficient - respect everyone's time (1-2 sentence responses)
‚Ä¢ Direct questions get direct answers: "May I ask who's calling?" "What's this regarding?"
‚Ä¢ Professional skepticism with spam - polite but firm dismissal
‚Ä¢ For legitimate callers, ALWAYS collect NAME before forwarding/booking
‚Ä¢ Natural conversation flow - don't sound scripted{calendar_context}

DECISION RULES:
‚Ä¢ Minimum 2 exchanges before FORWARD_CALL or END_CALL (gather sufficient context)
‚Ä¢ Collect caller's NAME before BOOK_MEETING or FORWARD_CALL
‚Ä¢ When in doubt between legitimate and spam ‚Üí ask ONE more clarifying question
‚Ä¢ Never forward obvious spam just to avoid confrontation
‚Ä¢ Never dismiss legitimate callers because they're brief or nervous

SPECIAL COMMANDS (use exactly as shown on NEW LINE after your response):
‚Ä¢ FORWARD_CALL - Connect caller to Kevin immediately (only when Kevin is available and call is legitimate)
‚Ä¢ BOOK_MEETING - Schedule caller for next available slot (only when Kevin is busy and call is legitimate)
‚Ä¢ END_CALL - Politely end the call (only for confirmed spam/scam)
‚Ä¢ MORE_INFO - Continue conversation to gather additional details (when legitimacy is unclear)

EXAMPLE CONVERSATIONS:

Example 1 - Obvious Spam:
Caller: "This is your final notice about your car's extended warranty‚Äî"
You: "I appreciate the call, but we're not interested. Have a good day.
END_CALL"

Example 2 - Legitimate, Kevin Available:
Caller: "Hi, this is Sarah from Acme Corp. Kevin and I spoke last week about the Q3 project."
You: "Thanks for calling, Sarah. Let me connect you with Kevin right now.
FORWARD_CALL"

Example 3 - Legitimate, Kevin Busy:
Caller: "This is John Chen, I'm following up on the proposal Kevin requested."
You: "Thanks, John. Kevin is currently in a meeting. I can book you for his next available 15-minute slot. Would that work?
BOOK_MEETING"

Example 4 - Need More Information:
Caller: "I was referred to Kevin about a potential collaboration."
You: "I'd be happy to help. Who referred you, and what type of collaboration were you hoping to discuss?
MORE_INFO"

Example 5 - Sophisticated Sales (needs verification):
Caller: "Hi, I'm calling from TechCo about enterprise solutions."
You: "May I ask who specifically at TechCo referred you to Kevin, or how you got his contact information?
MORE_INFO"

CRITICAL REMINDERS:
‚Ä¢ Kevin's time is his most valuable asset - protect it ruthlessly
‚Ä¢ Be courteous to everyone, but don't let politeness override judgment
‚Ä¢ Legitimate callers understand reasonable screening - they won't be offended
‚Ä¢ Trust your assessment - if it feels like spam, it probably is
‚Ä¢ Quality over quantity - one legitimate connected call > ten spam dismissals
"""
            }
        ]
        
        # Add conversation history
        messages.extend(conversation_history)
        
        # Add transcribed caller text as user message
        messages.append({
            "role": "user",
            "content": caller_transcription
        })
        
        # Call BosonAI text completion model to generate response
        response = await call_bosonai_with_retry(
            "chat.completions.create",
            model="Qwen3-14B-Hackathon",  # Use text model instead of audio understanding
            messages=messages,
            temperature=0.2,
        )
        
        if not response:
            log("Failed to get response from BosonAI (all API keys failed or timed out)")
            return None, None, None, None
        
        model_response = response.choices[0].message.content
        
        if not model_response:
            log("No response from BosonAI")
            return None, None, None, None
            
        log(f"ü§ñ AI response: {model_response}")
        
        # Strip out <think> tags if present (model's internal reasoning)
        # This prevents the thinking process from triggering spam detection
        import re
        thinking_pattern = r'<think>.*?</think>\s*'
        model_response_clean = re.sub(thinking_pattern, '', model_response, flags=re.DOTALL).strip()
        
        if model_response_clean != model_response:
            log(f"üß† Stripped thinking tags, clean response: {model_response_clean}")
        
        # Check for special commands (look for keywords that indicate spam/forward intent)
        action = None
        response_text = model_response_clean
        caller_name = None
        
        # IMPORTANT: Ignore FORWARD_CALL and END_CALL until at least 3 exchanges have occurred
        # This ensures the AI gathers enough information before routing the call
        
        # Check for explicit commands
        if "FORWARD_CALL" in model_response_clean:
            action = "FORWARD"
            response_text = model_response_clean.replace("FORWARD_CALL", "").strip()
            if exchange_count < MIN_EXCHANGES_BEFORE_ACTION:
                log(f"‚è∏Ô∏è FORWARD action detected but ignoring (exchange {exchange_count + 1}/{MIN_EXCHANGES_BEFORE_ACTION})")
                action = None  # Suppress action until minimum exchanges reached
            else:
                log("‚ö° ACTION: Forwarding call to Kevin's phone")
        elif "BOOK_MEETING" in model_response_clean:
            action = "BOOK"
            response_text = model_response_clean.replace("BOOK_MEETING", "").strip()
            log("‚ö° ACTION: Booking meeting at next available time")
            
            # Extract caller name from conversation history
            caller_name = extract_caller_name(conversation_history, caller_transcription)
            log(f"üìù Extracted caller name: {caller_name}")
        elif "END_CALL" in model_response_clean:
            action = "END"
            response_text = model_response_clean.replace("END_CALL", "").strip()
            if exchange_count < MIN_EXCHANGES_BEFORE_ACTION:
                log(f"‚è∏Ô∏è END action detected but ignoring (exchange {exchange_count + 1}/{MIN_EXCHANGES_BEFORE_ACTION})")
                action = None  # Suppress action until minimum exchanges reached
            else:
                log("‚ö° ACTION: Ending call (spam detected)")
        else:
            # Fallback: detect spam indicators in the response
            # Only check for spam when it's clearly the bot dismissing the call
            spam_indicators = [
                "end this call", "end the call", "ending this call",
                "cannot assist", "can't assist", "not legitimate", 
                "telemarkeeter", "robocall", "have a good day. goodbye",
                "need to end this", "dismiss this call"
            ]
            forward_indicators = [
                "connect you", "forward", "transfer you", "put you through",
                "let me get kevin", "patch you through"
            ]
            
            response_lower = response_text.lower()
            
            if any(indicator in response_lower for indicator in spam_indicators):
                action = "END"
                if exchange_count < MIN_EXCHANGES_BEFORE_ACTION:
                    log(f"‚è∏Ô∏è END action detected (keywords) but ignoring (exchange {exchange_count + 1}/{MIN_EXCHANGES_BEFORE_ACTION})")
                    action = None  # Suppress action until minimum exchanges reached
                else:
                    log("‚ö° ACTION: Ending call (spam keywords detected in response)")
            elif any(indicator in response_lower for indicator in forward_indicators):
                action = "FORWARD"
                if exchange_count < MIN_EXCHANGES_BEFORE_ACTION:
                    log(f"‚è∏Ô∏è FORWARD action detected (keywords) but ignoring (exchange {exchange_count + 1}/{MIN_EXCHANGES_BEFORE_ACTION})")
                    action = None  # Suppress action until minimum exchanges reached
                else:
                    log("‚ö° ACTION: Forwarding call (forward keywords detected in response)")

        
        # Add to conversation history (caller's transcribed text + assistant's response)
        conversation_history.append({
            "role": "user",
            "content": caller_transcription
        })
        conversation_history.append({
            "role": "assistant",
            "content": response_text
        })
        
        # Generate speech from text response (following example1.py pattern)
        log("Step 3: Generating speech from response...")
        
        # Remove content in brackets before TTS (stage directions, actions, etc.)
        # Remove [text], (text), and {text} patterns
        import re
        tts_text = re.sub(r'\[.*?\]|\(.*?\)|\{.*?\}', '', response_text).strip()
        log(f"TTS input (brackets removed): {tts_text}")
        
        speech_response = await call_bosonai_with_retry(
            "audio.speech.create",
            model="higgs-audio-generation-Hackathon",
            voice=VOICE,
            input=tts_text,
            response_format="pcm"
        )
        
        if not speech_response:
            log("Failed to generate speech from BosonAI (all API keys failed or timed out)")
            return None, None, None, None
        
        # Audio specs from example1.py
        # num_channels = 1
        # sample_width = 2
        # sample_rate = 24000
        
        pcm_data = speech_response.content
        
        log(f"Received {len(pcm_data)} bytes of PCM audio from BosonAI")
        
        # Convert PCM16 24kHz -> PCM16 8kHz for recording and Twilio
        pcm16_8k_full, _ = audioop.ratecv(pcm_data, 2, 1, 24000, 8000, None)
        
        # Write bot response to WAV file
        if wav_file:
            wav_file.writeframes(pcm16_8k_full)
        
        # Calculate audio duration for proper delay (PCM16 8kHz, mono, 2 bytes/sample)
        audio_duration_seconds = len(pcm16_8k_full) / (8000 * 2)
        log(f"Bot audio duration: {audio_duration_seconds:.2f} seconds")
        
        # Convert to Œº-law and send to Twilio in chunks
        # Process in chunks to avoid memory issues
        chunk_size = 1600  # 100ms at 8kHz (1 channel, 2 bytes/sample)
        for i in range(0, len(pcm16_8k_full), chunk_size):
            chunk = pcm16_8k_full[i:i + chunk_size]
            if len(chunk) < 4:  # Need at least 2 samples
                continue
            
            # Convert to Œº-law
            mulaw_8k = audioop.lin2ulaw(chunk, 2)
            
            # Send to Twilio
            await websocket.send_text(json.dumps({
                "event": "media",
                "streamSid": stream_sid,
                "media": {
                    "payload": base64.b64encode(mulaw_8k).decode("utf-8")
                }
            }))
        
        log("Finished streaming response to caller - will ignore audio during playback...")
        
        # Calculate exact audio duration: PCM16 8kHz, mono, 2 bytes per sample
        # Duration = total_bytes / (sample_rate * bytes_per_sample)
        audio_duration_seconds = len(pcm16_8k_full) / (8000 * 2)
        log(f"Bot audio duration: {audio_duration_seconds:.2f} seconds")
        
        # Total delay = exact audio playback time + fixed post-audio buffer
        total_delay_seconds = audio_duration_seconds + POST_AUDIO_DELAY_SECONDS
        log(f"Will block user input for {total_delay_seconds:.2f}s ({audio_duration_seconds:.2f}s audio + {POST_AUDIO_DELAY_SECONDS:.2f}s buffer)")
        
        # Execute action if needed
        if action == "FORWARD":
            log(f"üìû Forwarding call {call_sid} to {KEVIN_PHONE_NUMBER}")
            await forward_call(call_sid)
        elif action == "BOOK":
            log(f"üìÖ Booking meeting for {caller_name}")
            booking_result = book_next_available(caller_name, caller_phone="", caller_email="")
            log(f"‚úÖ Booking result: {booking_result}")
            
            # Send confirmation message to caller
            confirmation_text = f"Perfect! I've scheduled you for a 15-minute call with Kevin. {booking_result}. You'll receive a confirmation. Thank you for calling!"
            
            # Remove content in brackets before TTS
            import re
            confirmation_tts = re.sub(r'\[.*?\]|\(.*?\)|\{.*?\}', '', confirmation_text).strip()
            
            # Generate speech for confirmation
            speech_response = await call_bosonai_with_retry(
                "audio.speech.create",
                model="higgs-audio-generation-Hackathon",
                voice=VOICE,
                input=confirmation_tts,
                response_format="pcm"
            )
            
            if not speech_response:
                log("Failed to generate confirmation speech from BosonAI (all API keys failed or timed out)")
                # Still try to end the call even if speech failed
            else:
                pcm_data = speech_response.content
                pcm16_8k_confirmation, _ = audioop.ratecv(pcm_data, 2, 1, 24000, 8000, None)
                
                # Calculate confirmation audio duration
                confirmation_duration_seconds = len(pcm16_8k_confirmation) / (8000 * 2)
                log(f"Confirmation audio duration: {confirmation_duration_seconds:.2f} seconds")
                
                # Send confirmation to caller
                chunk_size = 1600
                for i in range(0, len(pcm16_8k_confirmation), chunk_size):
                    chunk = pcm16_8k_confirmation[i:i + chunk_size]
                    if len(chunk) < 4:
                        continue
                    mulaw_8k = audioop.lin2ulaw(chunk, 2)
                    await websocket.send_text(json.dumps({
                        "event": "media",
                        "streamSid": stream_sid,
                        "media": {
                            "payload": base64.b64encode(mulaw_8k).decode("utf-8")
                        }
                    }))
                
                # Wait for confirmation to play completely
                confirmation_duration_seconds = len(pcm16_8k_confirmation) / (8000 * 2)
                log(f"Confirmation will play for {confirmation_duration_seconds:.2f} seconds...")
                
                # Actually wait for the audio to finish playing
                import asyncio
                wait_time = confirmation_duration_seconds + 0.5  # Audio duration + small buffer
                log(f"‚è≥ Waiting {wait_time:.2f}s for confirmation to finish before ending call...")
                await asyncio.sleep(wait_time)
            
            # End the call after booking (and after confirmation finishes playing)
            await end_call(call_sid)
        elif action == "END":
            log(f"üö´ Ending call {call_sid} (spam)")
            # Wait for the goodbye message to finish playing before hanging up
            # Audio was already sent above, now wait for it to complete
            import asyncio
            wait_time = audio_duration_seconds + 0.5  # Audio duration + small buffer
            log(f"‚è≥ Waiting {wait_time:.2f}s for goodbye message to finish before ending call...")
            await asyncio.sleep(wait_time)
            await end_call(call_sid)
        
        # Return total delay duration for blocking user input
        return model_response, pcm16_8k_full, action, caller_transcription, total_delay_seconds
        
    except Exception as e:
        log(f"BosonAI error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, 0.0


async def forward_call(call_sid: str):
    """Forward the call to Kevin's personal phone number using Twilio API."""
    try:
        import httpx
        
        account_sid = os.getenv("TWILIO_ACCOUNT_SID")
        auth_token = os.getenv("TWILIO_AUTH_TOKEN")
        
        if not account_sid or not auth_token:
            log("‚ö†Ô∏è TWILIO_ACCOUNT_SID and TWILIO_AUTH_TOKEN must be set in .env to forward calls")
            return
        
        # Update the call to dial Kevin's number
        url = f"https://api.twilio.com/2010-04-01/Accounts/{account_sid}/Calls/{call_sid}.json"
        
        # TwiML to dial Kevin's number with extended timeout
        twiml = f"""<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Say>Please hold while I connect you to Kevin.</Say>
    <Dial timeout="30" callerId="{KEVIN_PHONE_NUMBER}">
        <Number>{KEVIN_PHONE_NUMBER}</Number>
    </Dial>
    <Say>I'm sorry, Kevin is not available right now. Please try again later. Goodbye.</Say>
</Response>"""
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url,
                auth=(account_sid, auth_token),
                data={"Twiml": twiml}
            )
            
            if response.status_code == 200:
                log(f"‚úÖ Call forwarded successfully to {KEVIN_PHONE_NUMBER}")
            else:
                log(f"‚ùå Failed to forward call: {response.status_code} - {response.text}")
                
    except Exception as e:
        log(f"Error forwarding call: {e}")


async def end_call(call_sid: str):
    """End the call immediately using Twilio API."""
    try:
        import httpx
        
        account_sid = os.getenv("TWILIO_ACCOUNT_SID")
        auth_token = os.getenv("TWILIO_AUTH_TOKEN")
        
        if not account_sid or not auth_token:
            log("‚ö†Ô∏è TWILIO_ACCOUNT_SID and TWILIO_AUTH_TOKEN must be set in .env to end calls")
            return
        
        # Update the call status to "completed" to end it
        url = f"https://api.twilio.com/2010-04-01/Accounts/{account_sid}/Calls/{call_sid}.json"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url,
                auth=(account_sid, auth_token),
                data={"Status": "completed"}
            )
            
            if response.status_code == 200:
                log(f"‚úÖ Call ended successfully")
            else:
                log(f"‚ùå Failed to end call: {response.status_code} - {response.text}")
                
    except Exception as e:
        log(f"Error ending call: {e}")


async def save_call_to_database(
    call_sid: str,
    from_number: str,
    conversation_log: list,
    transcripts: list,
    recording_filename: str,
    is_spam: bool
):
    """
    Save call information to the SQLiteCloud database.
    
    Args:
        call_sid: Twilio call SID
        from_number: Caller's phone number
        conversation_log: List of conversation exchanges
        transcripts: List of bot responses
        recording_filename: Path to the WAV recording file
        is_spam: Whether the call was identified as spam
    """
    log(f"[SAVE] Starting database save for call {call_sid}")
    log(f"[SAVE] Recording filename: {recording_filename}")
    log(f"[SAVE] From number: {from_number}")
    log(f"[SAVE] Is spam: {is_spam}")
    
    if not SQLITE_URL:
        log("‚ö†Ô∏è Database not configured - skipping save")
        return
    
    try:
        # Extract caller name and call description from conversation
        caller_name = "Unknown Caller"
        description = "No conversation"
        
        # Try to extract name from bot responses
        for entry in conversation_log:
            if entry.get('speaker') == 'Bot':
                text = entry.get('text', '')
                # Look for patterns like "Thank you [Name]" or "Hi [Name]"
                if 'thank you' in text.lower() or 'thanks' in text.lower() or 'hi ' in text.lower():
                    words = text.split()
                    for i, word in enumerate(words):
                        if word.lower() in ['thank', 'thanks', 'hi', 'hello'] and i + 1 < len(words):
                            potential_name = words[i + 1].strip('.,!?')
                            if potential_name and potential_name[0].isupper() and len(potential_name) > 1:
                                caller_name = potential_name
                                log(f"[SAVE] Extracted caller name: {caller_name}")
                                break
        
        # Create a concise, high-level description
        if is_spam:
            # For spam calls, create a short description
            spam_types = {
                'warranty': 'Car warranty scam',
                'irs': 'IRS scam call',
                'microsoft': 'Tech support scam',
                'computer': 'Tech support scam',
                'student loan': 'Student loan scam',
                'credit card': 'Credit card offer',
                'vacation': 'Vacation scam',
                'prize': 'Prize scam',
                'social security': 'Social Security scam'
            }
            
            # Check transcripts for spam keywords
            description = "Spam call"
            if transcripts:
                transcript_text = ' '.join(transcripts).lower()
                for keyword, desc in spam_types.items():
                    if keyword in transcript_text:
                        description = desc
                        break
        else:
            # For legitimate calls, extract the main topic
            if transcripts and len(transcripts) > 0:
                # Get the caller's stated purpose from the conversation
                first_response = transcripts[0].replace("FORWARD_CALL", "").replace("END_CALL", "").strip()
                
                # Extract key phrases
                if 'hackathon' in first_response.lower():
                    description = "Hackathon related inquiry"
                elif 'project' in first_response.lower():
                    description = "Project discussion"
                elif 'meeting' in first_response.lower():
                    description = "Meeting request"
                elif 'interview' in first_response.lower():
                    description = "Interview scheduled"
                else:
                    # Take first sentence as description (limit to 60 chars)
                    sentences = first_response.split('.')
                    if sentences:
                        description = sentences[0].strip()
                        if len(description) > 60:
                            description = description[:60] + "..."
        
        log(f"[SAVE] Caller: {caller_name}")
        log(f"[SAVE] Description: {description}")
        
        # Create Voicemail object
        voicemail = Voicemail(
            id=0,  # Will be auto-generated by database
            number=from_number if from_number and from_number != 'unknown' and from_number != 'Unknown' else 'Unknown Number',
            name=caller_name,
            description=description,
            spam=is_spam,
            date=datetime.now(timezone.utc),
            unread=True,
            recording=recording_filename
        )
        
        log(f"[SAVE] Calling add_row() to save to database...")
        # Save to database
        add_row(SQLITE_URL, voicemail)
        log(f"‚úÖ Call saved to database: {caller_name} ({from_number}) - Spam: {is_spam}")
        
    except Exception as e:
        log(f"‚ùå Failed to save call to database: {e}")
        import traceback
        traceback.print_exc()


class VadProcessor:
    """Handles VAD-based speech segmentation for endpointing."""
    
    def __init__(self, on_utterance_callback):
        self.vad = webrtcvad.Vad(VAD_MODE)
        self.on_utterance = on_utterance_callback
        self.buf_pcm16_8k = bytearray()
        self.sil_ms = 0
        self.speech_ms = 0
        self.utt_ms = 0
        self.in_speech = False
    
    def feed_mulaw_frame(self, mulaw_frame: bytes):
        """
        Feed a 20ms Œº-law frame (160 bytes) and check for utterance endpoint.
        mulaw_frame must be 20 ms at 8 kHz -> 160 samples -> 160 bytes of Œº-law.
        """
        if len(mulaw_frame) != 160:
            return  # Skip invalid frames
        
        # Œº-law ‚Üí PCM16 8 kHz
        pcm16_8k = audioop.ulaw2lin(mulaw_frame, 2)
        
        # VAD expects linear PCM at the same sample rate
        is_speech = self.vad.is_speech(pcm16_8k, sample_rate=8000)
        self.utt_ms += FRAME_MS
        
        if is_speech:
            self.in_speech = True
            self.speech_ms += FRAME_MS
            self.sil_ms = 0
            self.buf_pcm16_8k.extend(pcm16_8k)
        else:
            if self.in_speech:
                self.sil_ms += FRAME_MS
                self.buf_pcm16_8k.extend(pcm16_8k)  # Include trailing silence
        
        # Endpoint conditions
        if self.in_speech and (self.sil_ms >= END_SIL_MS or self.utt_ms >= MAX_UTT_MS):
            # Finalize utterance - upsample to 16 kHz for ASR
            pcm16_16k, _ = audioop.ratecv(bytes(self.buf_pcm16_8k), 2, 1, 8000, 16000, None)
            log(f"Utterance detected: {self.speech_ms}ms speech, {self.sil_ms}ms silence")
            self.on_utterance(pcm16_16k)
            # Reset
            self.buf_pcm16_8k.clear()
            self.sil_ms = self.speech_ms = self.utt_ms = 0
            self.in_speech = False


@app.post('/twiml')
async def return_twiml(request: Request):
    """Return TwiML to connect the call to our WebSocket for bidirectional streaming"""
    print("POST TwiML")
    
    # Get the host from the request to build the WebSocket URL
    host = request.headers.get('host', f'localhost:{HTTP_SERVER_PORT}')
    protocol = 'wss' if request.url.scheme == 'https' else 'ws'
    
    # Use <Connect> with BIDIRECTIONAL streaming (both_tracks) to allow bot to speak
    twiml = f"""<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Connect>
        <Stream url="{protocol}://{host}/media-stream" />
    </Connect>
</Response>"""
    
    return Response(content=twiml, media_type="application/xml")


async def send_greeting(websocket: WebSocket, stream_sid: str, wav_file=None):
    """Send initial greeting when call starts."""
    if not boson_clients:
        return
    
    try:
        greeting_text = "Hi, you've reached the office of kevin. How can I help you today?"
        log(f"Sending greeting: {greeting_text}")
        
        # Remove content in brackets before TTS
        import re
        greeting_tts = re.sub(r'\[.*?\]|\(.*?\)|\{.*?\}', '', greeting_text).strip()
        
        # Generate speech from greeting
        speech_response = await call_bosonai_with_retry(
            "audio.speech.create",
            model="higgs-audio-generation-Hackathon",
            voice=VOICE,
            input=greeting_tts,
            response_format="pcm"
        )
        
        if not speech_response:
            log("Failed to generate greeting from BosonAI (all API keys failed or timed out)")
            return None
        
        pcm_data = speech_response.content
        log(f"Received {len(pcm_data)} bytes of PCM audio for greeting")
        
        # Convert PCM16 24kHz -> PCM16 8kHz for recording and Twilio
        pcm16_8k_full, _ = audioop.ratecv(pcm_data, 2, 1, 24000, 8000, None)
        
        # Calculate greeting audio duration
        greeting_duration_seconds = len(pcm16_8k_full) / (8000 * 2)
        log(f"Greeting audio duration: {greeting_duration_seconds:.2f} seconds")
        
        # Write bot greeting to WAV file
        if wav_file:
            wav_file.writeframes(pcm16_8k_full)
        
        # Send to Twilio in chunks
        chunk_size = 1600  # 100ms at 8kHz
        for i in range(0, len(pcm16_8k_full), chunk_size):
            chunk = pcm16_8k_full[i:i + chunk_size]
            if len(chunk) < 4:
                continue
            
            mulaw_8k = audioop.lin2ulaw(chunk, 2)
            await websocket.send_text(json.dumps({
                "event": "media",
                "streamSid": stream_sid,
                "media": {
                    "payload": base64.b64encode(mulaw_8k).decode("utf-8")
                }
            }))
        
        log("Greeting sent - will ignore audio during playback...")
        
        # Calculate exact audio duration: PCM16 8kHz, mono, 2 bytes per sample
        # Duration = total_bytes / (sample_rate * bytes_per_sample)
        greeting_duration_seconds = len(pcm16_8k_full) / (8000 * 2)
        log(f"Greeting audio duration: {greeting_duration_seconds:.2f} seconds")
        
        # Total delay = exact audio playback time + fixed post-audio buffer
        total_delay_seconds = greeting_duration_seconds + POST_AUDIO_DELAY_SECONDS
        log(f"Will block user input for {total_delay_seconds:.2f}s ({greeting_duration_seconds:.2f}s audio + {POST_AUDIO_DELAY_SECONDS:.2f}s buffer)")
        
        return greeting_text, total_delay_seconds
        
    except Exception as e:
        log(f"Error sending greeting: {e}")
        return None, 0.0  # Return 0 duration on error


@app.websocket("/media-stream")
async def media_stream(websocket: WebSocket):
    """Handle Twilio media stream WebSocket connection with VAD-based endpointing and BosonAI streaming"""
    await websocket.accept()
    log("Connection accepted")
    
    count = 0
    has_seen_media = False
    wav_file = None
    call_sid = None
    stream_sid = None
    from_number = "unknown"
    call_start_time = datetime.now()
    final_action = None
    mulaw_buffer = bytearray()
    transcripts = []
    conversation_log = []  # Detailed conversation with caller and bot
    conversation_history = []  # Track full conversation
    greeting_sent = False
    exchange_count = 0  # Track number of caller-bot exchanges (greeting doesn't count)
    
    # Track bot speaking state to prevent VAD during bot responses
    bot_is_speaking = False
    bot_speaking_until = None  # Timestamp when bot will finish speaking + buffer delay
    bot_finished_time = None  # Track when bot finished speaking for debounce
    
    # Callback for when VAD detects a complete utterance
    async def on_utterance(pcm16_16k: bytes, speech_duration_ms: int):
        nonlocal bot_is_speaking, buf_pcm16_8k, sil_ms, speech_ms, utt_ms, in_speech, final_action, mulaw_buffer, exchange_count, bot_speaking_until, bot_finished_time
        
        # Ignore very short utterances (breath, noise, feedback)
        if speech_duration_ms < MIN_SPEECH_MS:
            log(f"Ignoring short utterance ({speech_duration_ms}ms < {MIN_SPEECH_MS}ms minimum)")
            return
        
        log(f"Processing valid utterance ({len(pcm16_16k)} bytes at 16kHz, {speech_duration_ms}ms speech)...")
        
        # Set flag to disable VAD during bot response
        bot_is_speaking = True
        bot_finished_time = None  # Clear debounce timer during processing
        
        # Clear VAD buffer immediately to prevent echo detection
        buf_pcm16_8k.clear()
        sil_ms = speech_ms = utt_ms = 0
        in_speech = False
        
        # Process with BosonAI and stream response back
        if stream_sid:  # Make sure we have a stream_sid
            from datetime import timedelta
            response, bot_audio, action, caller_text, delay_seconds = await process_utterance_and_respond(pcm16_16k, websocket, stream_sid, conversation_history, call_sid, wav_file, exchange_count=exchange_count)
            if response:
                transcripts.append(response)
                # Increment exchange counter (caller spoke + bot responded = 1 exchange)
                exchange_count += 1
                log(f"üìä Exchange count: {exchange_count}")
                # Track final action
                if action:
                    final_action = action
                # Log the exchange with caller transcription
                conversation_log.append({
                    "speaker": "Caller",
                    "duration_ms": speech_duration_ms,
                    "audio_size": len(pcm16_16k),
                    "text": caller_text if caller_text else None,
                    "timestamp": datetime.now().isoformat()
                })
                conversation_log.append({
                    "speaker": "Bot",
                    "text": clean_text_for_transcript(response),
                    "timestamp": datetime.now().isoformat()
                })
                
                # Save transcript after each exchange (overwrite same file)
                save_transcript(call_sid, from_number, conversation_log, call_start_time, datetime.now(), final_action, call_in_progress=True)
                
                # Set timestamp when bot will finish speaking (now + delay_seconds)
                bot_speaking_until = datetime.now() + timedelta(seconds=delay_seconds)
                log(f"üîá Blocking user input until {bot_speaking_until.strftime('%H:%M:%S.%f')[:-3]} ({delay_seconds:.2f}s from now)")
        else:
            log("Warning: stream_sid or call_sid not set yet, skipping utterance")
        
        # Re-enable VAD immediately (timestamp check handles blocking)
        bot_is_speaking = False
        
        # CRITICAL: Clear mulaw_buffer to discard any audio that came in during bot speaking
        # This prevents echo/noise from being processed as the next utterance
        mulaw_buffer.clear()
        
        # Clear VAD buffers again after bot response to ensure clean slate
        buf_pcm16_8k.clear()
        sil_ms = speech_ms = utt_ms = 0
        in_speech = False
        
        log(f"‚úÖ Bot response sent - user input blocked until {bot_speaking_until.strftime('%H:%M:%S.%f')[:-3] if bot_speaking_until else 'now'}")
    
    # Simple utterance detector (we'll handle VAD manually for async callback)
    vad = webrtcvad.Vad(VAD_MODE)
    buf_pcm16_8k = bytearray()
    sil_ms = 0
    speech_ms = 0
    utt_ms = 0
    in_speech = False
    
    try:
        while True:
            message = await websocket.receive_text()
            
            if message is None:
                log("No message received...")
                continue

            data = json.loads(message)
            
            if data['event'] == "connected":
                log("Connected Message received:", message)
            
            elif data['event'] == "start":
                log("Start Message received:", message)
                # Extract call metadata
                stream_sid = data.get('streamSid')
                call_sid = data['start'].get('callSid')
                
                # Extract caller info if available
                from_number = data['start'].get('customParameters', {}).get('from', 'unknown')
                call_start_time = datetime.now()
                log(f"Call from: {from_number}")
                
                # Create WAV file for this call (8 kHz native PSTN rate)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{RECORDINGS_DIR}/call_{call_sid}_{timestamp}.wav"
                recording_filename = f"call_{call_sid}_{timestamp}.wav"  # Store just the filename
                
                # Open WAV file with proper settings for mulaw -> PCM conversion
                wav_file = wave.open(filename, 'wb')
                wav_file.setnchannels(1)  # Mono
                wav_file.setsampwidth(2)  # 2 bytes (16-bit PCM)
                wav_file.setframerate(8000)  # 8000 Hz (native PSTN)
                
                log(f"Recording to WAV (8kHz): {filename}")
                log(f"VAD enabled: endpointing at {END_SIL_MS}ms silence, min speech {MIN_SPEECH_MS}ms")
                log(f"BosonAI bot ready")
            
            elif data['event'] == "media":
                # Always write to WAV for complete recording
                payload = data['media']['payload']
                mulaw_data = base64.b64decode(payload)
                if wav_file:
                    pcm_data = audioop.ulaw2lin(mulaw_data, 2)
                    wav_file.writeframes(pcm_data)
                
                # Skip VAD processing if bot is speaking (but keep recording above)
                if bot_is_speaking:
                    continue
                
                # Check if we're still in the blocking period after bot spoke
                if bot_speaking_until is not None:
                    import time
                    current_time = datetime.now()
                    if current_time < bot_speaking_until:
                        # Still in blocking period - skip VAD processing
                        continue
                    else:
                        # Blocking period over - clear the timer and resume normal processing
                        log(f"‚úÖ Blocking period over - resuming VAD at {current_time.strftime('%H:%M:%S.%f')[:-3]}")
                        bot_speaking_until = None
                
                if not has_seen_media:
                    log("Media message received - streaming audio with VAD...")
                    log("Additional media messages are being suppressed from logs...")
                    has_seen_media = True
                    
                    # Send greeting after first media packet (ensures stream is ready)
                    if not greeting_sent and stream_sid:
                        from datetime import timedelta
                        bot_is_speaking = True  # Prevent VAD during greeting
                        greeting_result = await send_greeting(websocket, stream_sid, wav_file)
                        
                        if greeting_result:
                            greeting, delay_seconds = greeting_result
                            if greeting:
                                conversation_history.append({
                                    "role": "assistant",
                                    "content": greeting
                                })
                                conversation_log.append({
                                    "speaker": "Bot",
                                    "text": clean_text_for_transcript(greeting),
                                    "timestamp": datetime.now().isoformat()
                                })
                                
                                # Save transcript after greeting (overwrite same file)
                                save_transcript(call_sid, from_number, conversation_log, call_start_time, datetime.now(), final_action, call_in_progress=True)
                            
                            # Set timestamp when bot will finish speaking (now + delay_seconds)
                            bot_speaking_until = datetime.now() + timedelta(seconds=delay_seconds)
                            log(f"üîá Blocking user input until {bot_speaking_until.strftime('%H:%M:%S.%f')[:-3]} ({delay_seconds:.2f}s from now)")
                        
                        # Re-enable VAD after greeting is sent (timestamp check handles blocking)
                        bot_is_speaking = False
                        
                        # Clear any accumulated audio during greeting
                        mulaw_buffer.clear()
                        buf_pcm16_8k.clear()
                        sil_ms = speech_ms = utt_ms = 0
                        in_speech = False
                        greeting_sent = True
                        continue  # Skip processing this packet
                
                payload = data['media']['payload']  # base64 encoded mulaw audio
                mulaw_data = base64.b64decode(payload)
                
                # Write caller audio (8kHz PCM) to WAV file for archival
                if wav_file:
                    pcm_data = audioop.ulaw2lin(mulaw_data, 2)
                    wav_file.writeframes(pcm_data)
                
                # Check if we're still in the blocking period (simple timestamp check)
                if bot_speaking_until and datetime.now() < bot_speaking_until:
                    # Still within the delay period - ignore all audio input
                    continue
                elif bot_speaking_until and datetime.now() >= bot_speaking_until:
                    # Just passed the blocking threshold - clear buffers and resume listening
                    log(f"‚úÖ Delay period complete - now listening for caller input")
                    bot_speaking_until = None
                    # Clear buffers to discard any accumulated audio/echo
                    mulaw_buffer.clear()
                    buf_pcm16_8k.clear()
                    sil_ms = speech_ms = utt_ms = 0
                    in_speech = False
                    continue
                
                # Add to buffer and process in 20ms frames (160 bytes Œº-law)
                # (mulaw_data already extracted at top of media event handler)
                mulaw_buffer.extend(mulaw_data)
                
                # Process complete 20ms frames for VAD
                while len(mulaw_buffer) >= 160:
                    frame = bytes(mulaw_buffer[:160])
                    mulaw_buffer = mulaw_buffer[160:]
                    
                    # Manual VAD processing for async callback support
                    pcm16_8k = audioop.ulaw2lin(frame, 2)
                    is_speech = vad.is_speech(pcm16_8k, sample_rate=8000)
                    utt_ms += FRAME_MS
                    
                    if is_speech:
                        in_speech = True
                        speech_ms += FRAME_MS
                        sil_ms = 0
                        buf_pcm16_8k.extend(pcm16_8k)
                    else:
                        if in_speech:
                            sil_ms += FRAME_MS
                            buf_pcm16_8k.extend(pcm16_8k)
                    
                    # Check for utterance endpoint
                    if in_speech and (sil_ms >= END_SIL_MS or utt_ms >= MAX_UTT_MS):
                        # Finalize utterance - upsample to 16 kHz for ASR
                        pcm16_16k, _ = audioop.ratecv(bytes(buf_pcm16_8k), 2, 1, 8000, 16000, None)
                        log(f"Utterance detected: {speech_ms}ms speech, {sil_ms}ms silence")
                        
                        # Process with BosonAI (async) - this will set bot_is_speaking=True
                        # Pass speech duration to filter out short noise
                        await on_utterance(pcm16_16k, speech_ms)
                        
                        # Reset VAD state
                        buf_pcm16_8k.clear()
                        sil_ms = speech_ms = utt_ms = 0
                        in_speech = False
            
            elif data['event'] == "stop":
                log("Stop Message received:", message)
                break
            
            elif data['event'] == "closed":
                log("Closed Message received:", message)
                break
            
            count += 1
    
    except WebSocketDisconnect:
        log("WebSocket disconnected")
    except Exception as e:
        log(f"Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        call_end_time = datetime.now()
        
        # Close the WAV file
        if wav_file:
            wav_file.close()
            log(f"WAV file saved successfully - ready to play!")
        
        # Save final transcript to file (mark as complete)
        if conversation_log and call_sid:
            save_transcript(call_sid, from_number, conversation_log, call_start_time, call_end_time, final_action, call_in_progress=False)
        # Determine if call was spam based on conversation
        is_spam = False
        for entry in conversation_log:
            if entry.get('speaker') == 'Bot':
                text = entry.get('text', '').lower()
                if 'spam' in text or 'end_call' in text or 'scam' in text:
                    is_spam = True
                    break
        
        # Save call to database
        if call_sid and recording_filename:
            await save_call_to_database(
                call_sid=call_sid,
                from_number=from_number,
                conversation_log=conversation_log,
                transcripts=transcripts,
                recording_filename=recording_filename,
                is_spam=is_spam
            )
        
        # Log full conversation
        if conversation_log:
            log(f"\n{'='*60}")
            log(f"CALL TRANSCRIPT - {len([x for x in conversation_log if x['speaker'] == 'Caller'])} exchanges")
            log(f"{'='*60}")
            for i, entry in enumerate(conversation_log, 1):
                if entry['speaker'] == 'Caller':
                    caller_text = entry.get('text', None)
                    duration = entry.get('duration_ms', 0)
                    audio_size = entry.get('audio_size', 0)
                    if caller_text:
                        log(f"[{i}] üìû Caller: \"{caller_text}\" ({duration}ms, {audio_size} bytes)")
                    else:
                        log(f"[{i}] üìû Caller: [Audio {duration}ms, {audio_size} bytes - no transcription]")
                else:
                    text = entry.get('text', '')
                    log(f"[{i}] ü§ñ Bot: {text}")
            log(f"{'='*60}\n")
        
        # Log transcripts (old format for backward compatibility)
        if transcripts:
            log(f"Call summary - {len(transcripts)} bot response(s):")
            for i, t in enumerate(transcripts, 1):
                log(f"  [{i}] {t}")
        
        log(f"Connection closed. Received a total of {count} messages")


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "running",
        "message": "Twilio Media Stream FastAPI Server",
        "endpoints": {
            "twiml": "/twiml (POST)",
            "websocket": "/media-stream (WebSocket)",
            "voicemails": "/voicemails (GET)",
            "voicemail_recording": "/voicemail/{id}/recording (GET)"
        },
        "database": "enabled" if SQLITE_URL else "disabled"
    }


@app.get("/voicemails")
async def get_voicemails():
    """Retrieve all voicemails from the database"""
    log("[API] GET /voicemails - Fetching voicemails from database")
    
    if not SQLITE_URL:
        log("[API] ERROR: Database not configured")
        return {"error": "Database not configured"}
    
    try:
        from database.db_actions import read_table
        voicemails = read_table(SQLITE_URL)
        
        log(f"[API] Retrieved {len(voicemails)} voicemails from database")
        
        # Convert to JSON-serializable format
        result = []
        for vm in voicemails:
            result.append({
                "id": vm.id,
                "number": vm.number,
                "name": vm.name,
                "description": vm.description,
                "spam": vm.spam,
                "date": vm.date.isoformat(),
                "unread": vm.unread,
                "recording": vm.recording
            })
        
        log(f"[API] Returning {len(result)} voicemails to frontend")
        return {"voicemails": result, "count": len(result)}
    except Exception as e:
        log(f"[API] ERROR fetching voicemails: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


@app.get("/voicemail/{voicemail_id}/recording")
async def get_voicemail_recording(voicemail_id: int):
    """Get the audio recording for a specific voicemail"""
    if not SQLITE_URL:
        return {"error": "Database not configured"}
    
    try:
        from database.db_actions import get_recording
        recording_bytes = get_recording(SQLITE_URL, voicemail_id)
        
        if recording_bytes:
            from fastapi.responses import Response
            return Response(content=recording_bytes, media_type="audio/wav")
        else:
            return {"error": "Recording not found"}
    except Exception as e:
        return {"error": str(e)}


if __name__ == '__main__':
    print(f"Server listening on: http://localhost:{HTTP_SERVER_PORT}")
    print("Starting FastAPI server with WebSocket support...")
    uvicorn.run(app, host="0.0.0.0", port=HTTP_SERVER_PORT)
